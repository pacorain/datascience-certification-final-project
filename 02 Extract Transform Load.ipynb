{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Scoring Job Relocation Opportunities - ETL Scripts\n",
    "\n",
    "Austin Rainwater\n",
    "\n",
    "---\n",
    "\n",
    "For this notebook, I have created some libraries to assist in building an Extract/Transform/Load Data Pipeline. This pipeline uses asyncio, so that multiple requests to different endpoints can be made at one time. The pipeline allows the full Extract/Transform/Load process to be run concurrently, instead of synchronously running each step and waiting for their results.\n",
    "\n",
    "The pipeline consists of an abstract `PipelineStep` class. Each step must define a `process_batch(self, batch)` coroutine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade sqlalchemy==1.3.22 pymysql==0.9.3 PyYAML aiohttp aiomysql==0.0.21\n",
    "\n",
    "from etl import PipelineStep, DataPipeline\n",
    "from warnings import warn\n",
    "from abc import ABC\n",
    "import xml.etree.ElementTree as xml\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import sqlalchemy as sa\n",
    "\n",
    "from aiomysql.sa import create_engine\n",
    "\n",
    "from sqlalchemy import (\n",
    "    Table,\n",
    "    Column,\n",
    "    MetaData,\n",
    "    String,\n",
    "    Numeric,\n",
    "    Integer\n",
    ")\n",
    "\n",
    "with open('secrets.yaml', 'r') as secrets_file:\n",
    "    secrets = yaml.safe_load(secrets_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PipelineStep Subclasses\n",
    "\n",
    "These are some PipelineStep subclasses I plan to use multiple times because I will be accessing specific APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaPipelineStep(PipelineStep, ABC):\n",
    "    request_counter = 0\n",
    "    wikipedia_url = 'https://en.wikipedia.org/w/api.php'\n",
    "    wikipedia_header = {\"User-Agent\": \n",
    "          'datascience jupyter notebook/0.2 '\n",
    "          '(https://github.com/pacorain/datascience-certification-final-project; '\n",
    "          'Austin Rainwater, paco@heckin.io)'}\n",
    "    max_batch_size = 1\n",
    "    async_batches = True\n",
    "    \n",
    "    def start(self):\n",
    "        self.session = aiohttp.ClientSession()\n",
    "        super().start()\n",
    "        \n",
    "    def stop(self):\n",
    "        super().stop()\n",
    "        loop = asyncio.get_running_loop()\n",
    "        loop.create_task(self.session.close())\n",
    "    \n",
    "    async def make_request(self, params):\n",
    "        async with self.session.get(self.wikipedia_url, params=params, headers=self.wikipedia_header) as request:\n",
    "            self.__class__.request_counter += 1\n",
    "            assert 200 <= request.status <= 299\n",
    "            response = await request.json()\n",
    "            if params['action'] == 'query' and 'continue' in response:\n",
    "                warn(\"A Continue was issued but not handled.\")\n",
    "                # I'm not sure how to integrate this, or if I will even need to.\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoursquarePipelineStep(PipelineStep, ABC):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabasePipelineStep(PipelineStep, ABC):\n",
    "    \"\"\"Pipeline step for handle data for the database for this project.\n",
    "    \n",
    "    async_batches are disabled here because I just have a simple MariaDB instance, on a single\n",
    "    disk, so running multiple queries will not likely run any faster--and may even run slower.\n",
    "    \"\"\"\n",
    "    async_batches = False\n",
    "    max_batch_size = 500\n",
    "    \n",
    "    engine = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "loop = asyncio.get_running_loop()\n",
    "DatabasePipelineStep.engine = await create_engine(**secrets['db_prod'], loop=loop)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the actual pipeline steps\n",
    "\n",
    "Now, I can use the code I've written to create a data pipeline. To test, I will start with Fort Wayne, IN like I did in the previous document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeCityNames(WikipediaPipelineStep):\n",
    "    \"\"\"First step: take incoming city names and normalize them according to the titles of\n",
    "    their pages on Wikipedia.\n",
    "\n",
    "    Changes batch size to 50 since Wikipedia supports _querying_ 50 pages at a time\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = 49\n",
    "\n",
    "    async def process_batch(self, city_names):\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"redirects\": 1,\n",
    "            \"titles\": \"|\".join(city_names)\n",
    "        }\n",
    "        try:\n",
    "            result = await self.make_request(params)\n",
    "            response = result['query']\n",
    "        except KeyError as e:\n",
    "            raise RuntimeError(f\"Response not valid: {result}\", city_names)\n",
    "        if 'redirects' in response:\n",
    "            for redirect in response['redirects']:\n",
    "                #TODO: Cache redirects\n",
    "                yield redirect['to']\n",
    "        for page in response['pages'].values():\n",
    "            if \"missing\" in page.keys():\n",
    "                warn(f\"The city {page['title']} was provided but is not available on Wikipedia, and has been skipped\")\n",
    "            if page['title'] in city_names:\n",
    "                # Original name was not redirected; yield the original name\n",
    "                yield page['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fort Wayne, Indiana']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = DataPipeline(NormalizeCityNames(), data=['Fort Wayne'])\n",
    "await pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseTree(WikipediaPipelineStep):\n",
    "    \"\"\"\n",
    "    For each incoming city name, attatch its Wikipedia parsetree\n",
    "    \"\"\"\n",
    "    async def process_batch(self, normalized_city_names):\n",
    "        for city in normalized_city_names:\n",
    "            params = {\n",
    "                \"action\": \"parse\",\n",
    "                \"format\": \"json\",\n",
    "                \"redirects\": 1,\n",
    "                \"prop\": \"parsetree\",\n",
    "                \"page\": city\n",
    "            }\n",
    "            raw_response = (await self.make_request(params))['parse']['parsetree']['*']\n",
    "            response = xml.canonicalize(raw_response, strip_text=True)\n",
    "            yield (city, xml.fromstring(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fort Wayne, Indiana', <Element 'root' at 0x7fbb86fa3db0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = DataPipeline(NormalizeCityNames(), ParseTree(), data=['Fort Wayne'])\n",
    "await pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetCitiesFromCounties(WikipediaPipelineStep):\n",
    "    \"\"\"\n",
    "    Figure out which of a city's navboxes is for the county, and use that to expand each\n",
    "    city into the cities in its county.\n",
    "\n",
    "    Leaves city and tree attached so that the tree for the original city is not obtained twice.\n",
    "    \"\"\"\n",
    "    async def process_batch(self, city_parsetrees):\n",
    "        for original_city, tree in city_parsetrees:\n",
    "            seat = None\n",
    "            cities = None\n",
    "            state = None\n",
    "            for template in self.get_navbox_templates(tree):\n",
    "                raw_response, template_page = await self.get_template_page(template)\n",
    "                county_root = template_page.find(\".//template[title='US county navigation box']\")\n",
    "                state_root = template_page.find(\".//template[title='US state navigation box']\")\n",
    "                if county_root is not None:\n",
    "                    seat = await self.get_seat(county_root)\n",
    "                    cities = await self.parse_cities(raw_response)\n",
    "                elif state_root is not None:\n",
    "                    state = await self.get_state(state_root)\n",
    "                if seat and state:\n",
    "                    break\n",
    "            for city in cities:\n",
    "                yield (original_city, tree, city, state, seat)\n",
    "\n",
    "    def get_navbox_templates(self, wiki_page_tree):\n",
    "        \"\"\"Finds the topic navigation boxes on the wiki page (usually at the bottom)\"\"\"\n",
    "        navboxes = wiki_page_tree.findall(\".//template[title='Navboxes']/part[name='list']/value/template/title\")\n",
    "        return ['Template:{}'.format(elem.text) for elem in navboxes]\n",
    "\n",
    "    async def get_template_page(self, template):\n",
    "        \"\"\"Gets the template that defines the navigation box.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tuple[str, ElementTree]\n",
    "            The first element is the raw XML returned by the WikiPedia API.\n",
    "            The second element is the stripped and parsed XML data.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"action\": \"parse\",\n",
    "            \"format\": \"json\",\n",
    "            \"redirects\": 1,\n",
    "            \"prop\": \"parsetree\",\n",
    "            \"page\": template\n",
    "        }\n",
    "        raw_response = (await self.make_request(params))['parse']['parsetree']['*']\n",
    "        response = xml.canonicalize(raw_response, strip_text=True)\n",
    "        return raw_response, xml.fromstring(response)\n",
    "        \n",
    "    async def get_seat(self, root):\n",
    "        \"\"\"Gets the seat (e.g. metropolis) for a specified area from the navbox\"\"\"\n",
    "        seat_name = root.find(\".//part[name='seat']/value\").text\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"redirects\": \"1\",\n",
    "            \"titles\": seat_name\n",
    "        }\n",
    "        response = await self.make_request(params)\n",
    "        page = list(response['query']['pages'].values())[0]\n",
    "        assert 'missing' not in page\n",
    "        return page['title']\n",
    "\n",
    "        \n",
    "    async def parse_cities(self, raw_response_txt):\n",
    "        \"\"\"Using the raw XML, gets the cities from the navbox because they will be formatted as a list.\"\"\"\n",
    "        listed_city = re.compile(r\"\"\"\n",
    "            ^\\* \\ *       # Line starts with \"*\" plus any number of spaces\n",
    "            \\[{2}         # Start of link \"[[\"\n",
    "                ([^\\|]+)  # First part of link (between \"[[\" and \"|\"). This is the part that gets captured.\n",
    "                \\|        # Separator \"|\"\n",
    "                [^\\|]+    # Second part of link (between \"|\" and \"]]\")\n",
    "            \\]{2}‡?       # End of link \"]]\" plus optional ‡ character\n",
    "            \\ *$          # End with any number of spaces\n",
    "        \"\"\", re.VERBOSE + re.MULTILINE)\n",
    "        return listed_city.findall(raw_response_txt)\n",
    "    \n",
    "    async def get_state(self, root):\n",
    "        \"\"\"Gets the canonical state name for a city from the navbox\"\"\"\n",
    "        return root.find(\".//part[name='template_name']/value\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7ab28c20>,\n",
       "  'Fort Wayne, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana'),\n",
       " ('Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7ab28c20>,\n",
       "  'New Haven, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana'),\n",
       " ('Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7ab28c20>,\n",
       "  'Woodburn, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = DataPipeline(NormalizeCityNames(), ParseTree(), GetCitiesFromCounties(), data=['Fort Wayne'])\n",
    "await pipeline.run()\n",
    "pipeline.results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = NormalizeCityNames()\n",
    "parse = ParseTree()\n",
    "\n",
    "class SecondNormalizeStep(NormalizeCityNames):\n",
    "    \"\"\"Subclass of NormalizeCityNames, which is modified to accept and yield back the city's county seat\"\"\"\n",
    "    max_batch_size = 49\n",
    "    def __init__(self, first_step):\n",
    "        super().__init__()\n",
    "        self._duplicate_cache = first_step._duplicate_cache\n",
    "    \n",
    "    async def process_batch(self, batch):\n",
    "        cities, states, seats = zip(*batch)\n",
    "        i = 0\n",
    "        async for normalized_city in super().process_batch(cities):\n",
    "            yield normalized_city, states[i], seats[i]\n",
    "            i += 1\n",
    "            \n",
    "    def is_duplicate(self, record):\n",
    "        city = record[0]\n",
    "        return super().is_duplicate(city)\n",
    "        \n",
    "    \n",
    "class SecondParseTreeStep(ParseTree):\n",
    "    \"\"\"Subclass of ParseTree, which is modified to accept and yield back the city's county seat\"\"\"\n",
    "    def __init__(self, first_step):\n",
    "        super().__init__()\n",
    "        self._duplicate_cache = first_step._duplicate_cache\n",
    "        \n",
    "    async def process_batch(self, batch):\n",
    "        norm_cities, states, seats = zip(*batch)\n",
    "        i = 0\n",
    "        async for city_name, tree in super().process_batch(norm_cities):\n",
    "            yield city_name, states[i], seats[i], tree\n",
    "            \n",
    "    def is_duplicate(self, record):\n",
    "        city = record[0]\n",
    "        return super().is_duplicate(city)\n",
    "        \n",
    "\n",
    "class NewCitySplit(PipelineStep):\n",
    "    \"\"\"Runs the same processing on new cities in the county.\n",
    "    \n",
    "    It's designed specifically not to process the original city by creating a \"split\":\n",
    "    \n",
    "        incoming city matches original?\n",
    "          |          \\\n",
    "         yes          no\n",
    "          |            \\\n",
    "          |             |\n",
    "          |             v\n",
    "          |         normalize\n",
    "          |             |\n",
    "          |             v\n",
    "          |           parse\n",
    "          |             |\n",
    "          v             v\n",
    "         yield        yield\n",
    "           \\            /\n",
    "            \\          /\n",
    "             \\        /\n",
    "             |       |\n",
    "             v       v\n",
    "         NewCitySplit.outputs\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, normalize_step, parse_step):\n",
    "        super(NewCitySplit, self).__init__()\n",
    "        self.normalize_step = SecondNormalizeStep(normalize_step)\n",
    "        self.parse_step = SecondParseTreeStep(parse_step)\n",
    "        self.normalize_step.attach(self.parse_step)\n",
    "        self.parse_step.outputs = self.outputs\n",
    "    \n",
    "    async def process_batch(self, batch):\n",
    "        for original_city, tree, city, state, seat in batch:\n",
    "            if city == original_city:\n",
    "                yield city, state, seat, tree\n",
    "            else:\n",
    "                self.normalize_step.put((city, state, seat))\n",
    "        # Don't mark this task as \"complete\" until the pipeline step are done\n",
    "        await self.normalize_step.join()\n",
    "        await self.parse_step.join()\n",
    "    \n",
    "    def start(self):\n",
    "        super().start()\n",
    "        self.normalize_step.start()\n",
    "        self.parse_step.start()\n",
    "        \n",
    "        \n",
    "    def stop(self):\n",
    "        super().stop()\n",
    "        self.normalize_step.stop()\n",
    "        self.parse_step.stop()\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fort Wayne, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7aabeb30>),\n",
       " ('Aboite, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7a4ecae0>),\n",
       " ('Aboite Township, Allen County, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7a49ab80>),\n",
       " ('Academie, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7a4a4b30>),\n",
       " ('Adams Township, Allen County, Indiana',\n",
       "  'Indiana',\n",
       "  'Fort Wayne, Indiana',\n",
       "  <Element 'root' at 0x7fbb7a454040>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = NormalizeCityNames()\n",
    "parse = ParseTree()\n",
    "\n",
    "pipeline = DataPipeline(normalize, parse, GetCitiesFromCounties(), NewCitySplit(normalize, parse), data=['Fort Wayne, IN'])\n",
    "wikipedia_results = await pipeline.run()\n",
    "wikipedia_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiParsing(PipelineStep):\n",
    "    \"\"\"\n",
    "    Extracts data from XML ElementTree to put into database or use with FourSquare API.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    put(record):\n",
    "        Adds a record. The record should be a Tuple of the following elements:\n",
    "        \n",
    "            1. city (string): The canonical name of the city; e.g. `'New Haven, Indiana'`\n",
    "            2. state (string): The name of the state; e.g. `'Indiana'`\n",
    "            3. seat (string): The county seat; e.g. `'Fort Wayne, Indiana'`\n",
    "            4. tree (xml.etree.ElementTree): The parsed XML from Wikipedia for the city\n",
    "    \"\"\"\n",
    "    async_batches = True\n",
    "    batch_size = None\n",
    "    \n",
    "    async def process_batch(self, cities):\n",
    "        for city, state, seat, tree in cites:\n",
    "            latitude, longitude = self.parse_settlement_coords(tree)\n",
    "            yield (city, {\n",
    "                'city_name': city,\n",
    "                'metro_name': seat,\n",
    "                'state_name': state,\n",
    "                'center_latitude': latitude,\n",
    "                'center_longitude': longitude,\n",
    "                'area_val': float(self.infobox_value(tree, \"area_total_sq_mi\")),\n",
    "                'city_population': int(self.infobox_value(tree, \"population_est\")),\n",
    "                'population_density': float(self.infobox_value(tree, \"population_density_sq_mi\"))\n",
    "            }, self.get_weather_table(tree) if city == seat else None, self.get_population_history(tree))\n",
    "            \n",
    "    def is_duplicate(self, record):\n",
    "        city = record[0]\n",
    "        return super().is_duplicate(city)\n",
    "    \n",
    "    def parse_settlement_coordinates(wiki_data):\n",
    "        coords = wiki_data.findall(\".//part[name='coordinates']/value/template[title='coord']/part/value\")\n",
    "        # Convert from DMS (degrees, minutes, seconds) to Decimal\n",
    "        lat_deg, lat_min, lat_sec, lat_pole, lng_deg, lng_min, lng_sec, lng_pole = [x.text for x in coords[:8]]\n",
    "        lat_sign = 1 if lat_pole == 'N' else -1\n",
    "        lng_sign = 1 if lng_pole == 'E' else -1\n",
    "        latitude = sum([float(lat_deg), float(lat_min)/60.0, float(lat_sec)/3600.0]) * lat_sign\n",
    "        longitude = sum([float(lng_deg), float(lng_min)/60.0, float(lng_sec)/3600.0]) * lng_sign\n",
    "        return latitude, longitude\n",
    "    \n",
    "    def infobox_value(self, tree, part_name):\n",
    "        template = wiki_data.find(\".//template[title='Infobox settlement']\")\n",
    "        return template.find(\".part[name='{}'].value\".format(part_name)).text\n",
    "    \n",
    "    def get_weather_table(self, tree):\n",
    "        weather_box = wiki_data.find(\".//template[title='Weather box']\")\n",
    "        if weather_box is None:\n",
    "            return None\n",
    "        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'year']\n",
    "        stat_names = {\n",
    "            'record high F': 'record_high',\n",
    "            'avg record high F': 'avg_record_high',\n",
    "            'high F': 'avg_high',\n",
    "            'low F': 'avg_low',\n",
    "            'avg record low F': 'avg_record_low',\n",
    "            'record low F': 'record_low',\n",
    "            'precipitation inch': 'avg_precip',\n",
    "            'snow inch': 'avg_snow',\n",
    "            'precipitation days': 'precip_days',\n",
    "            'snow days': 'snow_days',\n",
    "            'sun': 'sunshine_hours',\n",
    "            'percentsun': 'daily_sunshine'\n",
    "        }\n",
    "\n",
    "        series_list = []\n",
    "        for month in months:\n",
    "            data = {}\n",
    "            for stat in stat_names.keys():\n",
    "                elem = weather_box.find(f\".//part[name='{month} {stat}'].value\")\n",
    "                if elem is None:\n",
    "                    val = np.nan\n",
    "                else:\n",
    "                    val = float(elem.text.replace('−', '-')) # The dashes in the data are not standard dashes for some reason\n",
    "                data[stat_names[stat]] = val\n",
    "            series_list.append(\n",
    "                pd.Series(data=data.values(), index=data.keys(), name=month)\n",
    "            )\n",
    "        return pd.DataFrame(series_list)\n",
    "    \n",
    "    def get_population_history(self, tree):\n",
    "        census = tree.find(\".//template[title='US Census population']\")\n",
    "        if census is None:\n",
    "            return\n",
    "\n",
    "        data = []\n",
    "        index = []\n",
    "\n",
    "        for part in census.findall(\"part\"):\n",
    "            year = part.find(\"name\").text\n",
    "            if year.isnumeric() and len(year) == 4:\n",
    "                data.append(int(part.find(\"value\").text))\n",
    "                index.append(year)\n",
    "\n",
    "        return pd.Series(\n",
    "            data, \n",
    "            pd.MultiIndex.from_arrays(\n",
    "                [[city_name] * len(index), index], names=['city', 'year']\n",
    "            ), \n",
    "            name='census_population'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaDatabaseStep(DatabasePipelineStep):\n",
    "    async def process_batch(self, batch):\n",
    "        citites, city_records, weather_tables, population_tables = zip(*batch)\n",
    "        for city in citites:\n",
    "            yield city\n",
    "        await self.insert_citites(city_records)\n",
    "        await self.insert_weather_data(citites, weather_tables)\n",
    "        await self.insert_population_history(citites, population_tables)\n",
    "        \n",
    "    meta = MetaData()\n",
    "        \n",
    "    city_table = Table(\"citites\", self.meta,\n",
    "        Column('city_name', String(50), primary_key=True, comment='City Name'),\n",
    "        Column('metro_name', String(50), comment='Metropolitan Area Name'),\n",
    "        Column('state_name', String(25), nullable=False, comment='State Name'),\n",
    "        Column('center_latitude', Numeric(10, 6), nullable=False, comment='Latitude of City center'),\n",
    "        Column('center_longitude', Numeric(10, 6), nullable=False, comment='Longitude of City center'),\n",
    "        Column('area_val', Numeric(10, 4), nullable=False, comment='Area of city in square miles'),\n",
    "        Column('city_population', Integer, nullable=False, comment='Total population of city'),\n",
    "        Column('population_density', Numeric(10, 4), comment='Population Density per square mile')\n",
    "    )\n",
    "    \n",
    "    weather_table = Table(\"metro_weather_data\")\n",
    "        \n",
    "    async def insert_cities(self, city_records):\n",
    "        async with self.engine.acquire() as conn:\n",
    "            await conn.execute(\n",
    "                self.city_table.insert(),\n",
    "                city_records\n",
    "            )\n",
    "    \n",
    "    async def insert_weather_data(self, cities, weather_tables):\n",
    "        dfs = dict(cities, weather_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
